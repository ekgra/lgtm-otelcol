---
grafana:
  # -- Deploy Grafana if enabled. See [upstream readme](https://github.com/grafana/helm-charts/tree/main/charts/grafana#configuration) for full values reference.
  enabled: true
  serviceAccount:
    create: false
    name: lgtm

  # Pin Grafana image to match subchart defaults
  image:
    registry: docker.io
    repository: grafana/grafana
    tag: "10.4.3"

  # Small footprint resources for Grafana
  resources:
    requests:
      cpu: 50m
      memory: 128Mi
    limits:
      cpu: 200m
      memory: 128Mi

  # -- Grafana data sources config. Connects to all three by default
  datasources:
    datasources.yaml:
      apiVersion: 1
      # -- Datasources linked to the Grafana instance. Override if you disable any components.
      datasources:
        # Loki datasource points to Query Frontend (gateway disabled)
        - name: Loki
          uid: loki
          type: loki
          url: http://{{ .Release.Name }}-loki-query-frontend:3100
          isDefault: false
        # Mimir (Prometheus) datasource pointing to query-frontend (no gateway)
        - name: Mimir
          uid: prom
          type: prometheus
          url: http://{{ .Release.Name }}-mimir-query-frontend:8080/prometheus
          isDefault: true
          jsonData:
            httpHeaderName1: "X-Scope-OrgID"
          secureJsonData:
            httpHeaderValue1: "tenant-1"
        # # https://grafana.com/docs/grafana/latest/datasources/prometheus/#provision-the-data-source
        # - name: Mimir
        #   uid: prom
        #   type: prometheus
        #   url: http://{{ .Release.Name }}-mimir-nginx/prometheus
        #   isDefault: true
        # https://grafana.com/docs/grafana/latest/datasources/tempo/configure-tempo-data-source/#provision-the-data-source
        - name: Tempo
          uid: tempo
          type: tempo
          url: http://{{ .Release.Name }}-tempo-query-frontend:3200
          isDefault: false
          jsonData:
            tracesToLogsV2:
              datasourceUid: loki
            lokiSearch:
              datasourceUid: loki
            tracesToMetrics:
              datasourceUid: prom
            serviceMap:
              datasourceUid: prom

loki:
  # -- Enable Loki (using the 'loki' chart, v6.9.0 / app 3.1.0)
  enabled: false
  # Use distributed mode to run specific microservices only
  deploymentMode: Distributed
  # Pin Loki image to match subchart defaults (used by all core components)
  image:
    registry: docker.io
    repository: grafana/loki
    tag: "3.1.0"
  # Reuse shared service account like other components
  serviceAccount:
    create: false
    name: lgtm
  # Disable the bundled gateway; we will not run it
  gateway:
    enabled: false
  # Reduce replication for small, single-node POC
  commonConfig:
    replication_factor: 1
  # Provide a proper TSDB schema (required in distributed/simple-scalable)
  schemaConfig:
    configs:
      - from: 2024-01-01
        store: tsdb
        object_store: s3
        schema: v13
        index:
          prefix: index_
          period: 24h
  # Nested config block required by subchart validation and config templating
  loki:
    # Disable multi-tenant auth; accept writes without X-Scope-OrgID
    auth_enabled: false
    commonConfig:
      replication_factor: 1
    schemaConfig:
      configs:
        - from: 2024-01-01
          store: tsdb
          object_store: s3
          schema: v13
          index:
            prefix: index_
            period: 24h
  # Disable all memcached caches
  resultsCache:
    enabled: false
  chunksCache:
    enabled: false
  memcachedExporter:
    enabled: false
  # Use embedded MinIO for object storage
  minio:
    enabled: true
    replicas: 1
    persistence:
      size: 5Gi
    resources:
      requests:
        cpu: 50m
        memory: 128Mi
  # Disable optional components we donâ€™t want
  ruler:
    enabled: false
  # TSDB components required for schema above
  indexGateway:
    replicas: 1
    resources:
      requests:
        cpu: 50m
        memory: 128Mi
      limits:
        cpu: 200m
        memory: 128Mi
  compactor:
    replicas: 1
    resources:
      requests:
        cpu: 50m
        memory: 128Mi
      limits:
        cpu: 200m
        memory: 128Mi
  # Enable only these components with minimal replicas
  distributor:
    replicas: 1
    resources:
      requests:
        cpu: 50m
        memory: 128Mi
      limits:
        cpu: 200m
        memory: 128Mi
  ingester:
    replicas: 1
    zoneAwareReplication:
      enabled: false
    resources:
      requests:
        cpu: 50m
        memory: 128Mi
      limits:
        cpu: 200m
        memory: 128Mi
  querier:
    replicas: 1
    resources:
      requests:
        cpu: 50m
        memory: 128Mi
      limits:
        cpu: 200m
        memory: 128Mi
  queryFrontend:
    replicas: 1
    resources:
      requests:
        cpu: 50m
        memory: 128Mi
      limits:
        cpu: 200m
        memory: 128Mi
  # Query scheduler is required for Distributed mode
  queryScheduler:
    replicas: 1
  # Ensure SimpleScalable targets are disabled when in Distributed mode
  write:
    replicas: 0
  read:
    replicas: 0
  backend:
    replicas: 0

  # Disable Loki canary since gateway is disabled and we don't need it
  lokiCanary:
    enabled: false
  # Disable Helm tests in the Loki subchart
  test:
    enabled: false

# -- Mimir chart values. Resources are set to a minimum by default.
mimir:
  # -- Deploy Mimir if enabled. See [upstream values.yaml](https://github.com/grafana/mimir/blob/main/operations/helm/charts/mimir-distributed/values.yaml) for full values reference.
  enabled: true
  image:
    repository: grafana/mimir
    tag: "2.16.0"
  # Nested config block used by the subchart to build mimir.yaml
  mimir:
    structuredConfig:
      ingester:
        ring:
          replication_factor: 1
      limits:
        out_of_order_time_window: 10m
  serviceAccount:
    create: false
    name: lgtm  
  alertmanager:
    enabled: false
    replicas: 0
    statefulSet:
      enabled: false
  compactor:
    enabled: false
  distributor:
    replicas: 1
    resources:
      requests:
        cpu: 50m
        memory: 128Mi
      limits:
        cpu: 200m
        memory: 128Mi
  ingester:
    replicas: 1
    zoneAwareReplication:
      enabled: false
    resources:
      requests:
        cpu: 50m
        memory: 128Mi
      limits:
        cpu: 200m
        memory: 128Mi
  overrides_exporter:
    enabled: false
    replicas: 0
  querier:
    replicas: 1
    resources:
      requests:
        cpu: 50m
        memory: 128Mi
      limits:
        cpu: 200m
        memory: 128Mi
  query_frontend:
    replicas: 1
    resources:
      requests:
        cpu: 50m
        memory: 128Mi
      limits:
        cpu: 200m
        memory: 128Mi
  query_scheduler:
    enabled: false
    replicas: 0
  ruler:
    enabled: false
    replicas: 0
  store_gateway:
    replicas: 1
    zoneAwareReplication:
      enabled: false
    resources:
      requests:
        cpu: 50m
        memory: 128Mi
      limits:
        cpu: 200m
        memory: 128Mi
  minio:
    enabled: true
    persistence:
      size: 5Gi
    resources:
      requests:
        cpu: 50m
        memory: 128Mi

  # Disable gateway (nginx)
  nginx:
    enabled: false

  # Disable memcached-based caches (chart 5.7.0 keys)
  chunks-cache:
    enabled: false
  index-cache:
    enabled: false
  metadata-cache:
    enabled: false
  results-cache:
    enabled: false
  rollout_operator:
    enabled: false
    resources:
      requests:
        cpu: 20m

tempo:
  # -- Deploy Tempo if enabled.  See [upstream readme](https://github.com/grafana/helm-charts/blob/main/charts/tempo-distributed/README.md#values) for full values reference.
  enabled: false
  serviceAccount:
    create: false
    name: lgtm
  # Pin Tempo image to match subchart defaults (used by all core components)
  image:
    registry: docker.io
    repository: grafana/tempo
    tag: "2.8.0"
  # Components to run
  ingester:
    replicas: 1
    config:
      # Single-ingester POC: write without replication
      replication_factor: 1
    resources:
      requests:
        cpu: 50m
        memory: 128Mi
      limits:
        cpu: 200m
        memory: 128Mi
  distributor:
    replicas: 1
    resources:
      requests:
        cpu: 50m
        memory: 128Mi
      limits:
        cpu: 200m
        memory: 128Mi
  querier:
    replicas: 1
    resources:
      requests:
        cpu: 50m
        memory: 128Mi
      limits:
        cpu: 200m
        memory: 128Mi
  queryFrontend:
    replicas: 1
    resources:
      requests:
        cpu: 50m
        memory: 128Mi
      limits:
        cpu: 200m
        memory: 128Mi
  # Enable OTLP ingestion on distributor (gRPC/HTTP)
  traces:
    otlp:
      grpc:
        enabled: true
      http:
        enabled: true
  # Disable memcached and clear caches
  memcached:
    enabled: false
  cache:
    caches: []

grafana-oncall:
  # -- Deploy Grafana OnCall if enabled. See [upstream values.yaml](https://github.com/grafana/oncall/blob/dev/helm/oncall/values.yaml) for full values reference.
  enabled: false

# OpenTelemetry Collector subchart values
otelcol:
  enabled: true
  serviceAccount:
    create: false
    name: lgtm
  mode: deployment
  image:
    repository: ghcr.io/open-telemetry/opentelemetry-collector-releases/opentelemetry-collector-k8s
    # tag left empty to use chart appVersion (0.134.1)
    tag: ""
  command:
    name: otelcol-k8s
  # Minimal config to accept OTLP and export traces to Tempo distributor
  config:
    exporters:
      otlphttp/tempo:
        endpoint: "http://{{ .Release.Name }}-tempo-distributor:4318"
        tls:
          insecure: true
      # Send logs to Loki via OTLP/HTTP (Loki 3.x)
      otlphttp/loki:
        # Base endpoint (no signal path appended)
        endpoint: "http://{{ .Release.Name }}-loki-distributor:3100"
        # Loki expects HTTP OTLP at /otlp/v1/logs
        logs_endpoint: "/otlp/v1/logs"
        tls:
          insecure: true
      # Send metrics to Mimir via native OTLP/HTTP
      otlphttp/mimir:
        endpoint: "http://{{ .Release.Name }}-mimir-distributor:8080/otlp"
        headers:
          X-Scope-OrgID: "tenant-1"
    service:
      pipelines:
        traces:
          exporters:
            - otlphttp/tempo
            - debug
        # Receive OTLP logs (from internal telemetry) and push to Loki
        logs:
          receivers:
            - otlp
          processors:
            - batch
          exporters:
            - otlphttp/loki
        # Receive OTLP + scrape self metrics; export to Mimir
        metrics:
          receivers:
            - otlp
            - prometheus
          processors:
            - memory_limiter
            - batch
          exporters:
            - otlphttp/mimir
  # Export the Collector's own traces to Tempo via OTLP/HTTP
  # internalTelemetryViaOTLP:
    # traces:
    #   enabled: true
    #   endpoint: "http://{{ .Release.Name }}-tempo-distributor:4318"
    # logs:
    #   enabled: true
    #   # Loop internal logs back to this collector's OTLP receiver
    #   # endpoint: "http://${env:MY_POD_IP}:4318"
    #   endpoint: "http://{{ .Release.Name }}-loki-distributor:3100/otlp/v1/logs"
  # Give self-telemetry a friendly service name
  extraEnvs:
    - name: OTEL_SERVICE_NAME
      value: "{{ .Release.Name }}-otelcol"
  # Small footprint resources for OpenTelemetry Collector
  resources:
    requests:
      cpu: 50m
      memory: 128Mi
    limits:
      cpu: 200m
      memory: 128Mi
