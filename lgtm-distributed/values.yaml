---
grafana:
  # -- Deploy Grafana if enabled. See [upstream readme](https://github.com/grafana/helm-charts/tree/main/charts/grafana#configuration) for full values reference.
  enabled: true
  serviceAccount:
    create: false
    name: lgtm

  # Pin Grafana image to match subchart defaults
  image:
    registry: docker.io
    repository: grafana/grafana
    tag: "10.4.3"

  # Small footprint resources for Grafana
  resources:
    requests:
      cpu: 50m
      memory: 128Mi
    limits:
      cpu: 200m
      memory: 128Mi

  # -- Grafana data sources config. Connects to all three by default
  datasources:
    datasources.yaml:
      apiVersion: 1
      # -- Datasources linked to the Grafana instance. Override if you disable any components.
      datasources:
        # https://grafana.com/docs/grafana/latest/datasources/loki/#provision-the-loki-data-source
        # - name: Loki
        #   uid: loki
        #   type: loki
        #   url: http://{{ .Release.Name }}-loki-gateway
        #   isDefault: false
        # # https://grafana.com/docs/grafana/latest/datasources/prometheus/#provision-the-data-source
        # - name: Mimir
        #   uid: prom
        #   type: prometheus
        #   url: http://{{ .Release.Name }}-mimir-nginx/prometheus
        #   isDefault: true
        # https://grafana.com/docs/grafana/latest/datasources/tempo/configure-tempo-data-source/#provision-the-data-source
        - name: Tempo
          uid: tempo
          type: tempo
          url: http://{{ .Release.Name }}-tempo-query-frontend:3200
          isDefault: false
          jsonData:
            tracesToLogsV2:
              datasourceUid: loki
            lokiSearch:
              datasourceUid: loki
            tracesToMetrics:
              datasourceUid: prom
            serviceMap:
              datasourceUid: prom

loki:
  # -- Deploy Loki if enabled. See [upstream readme](https://github.com/grafana/helm-charts/tree/main/charts/loki-distributed#values) for full values reference.
  enabled: false

# -- Mimir chart values. Resources are set to a minimum by default.
mimir:
  # -- Deploy Mimir if enabled. See [upstream values.yaml](https://github.com/grafana/mimir/blob/main/operations/helm/charts/mimir-distributed/values.yaml) for full values reference.
  enabled: false
  alertmanager:
    resources:
      requests:
        cpu: 20m
  compactor:
    resources:
      requests:
        cpu: 20m
  distributor:
    resources:
      requests:
        cpu: 20m
  ingester:
    replicas: 2
    zoneAwareReplication:
      enabled: false
    resources:
      requests:
        cpu: 20m
  overrides_exporter:
    resources:
      requests:
        cpu: 20m
  querier:
    replicas: 1
    resources:
      requests:
        cpu: 20m
  query_frontend:
    resources:
      requests:
        cpu: 20m
  query_scheduler:
    replicas: 1
    resources:
      requests:
        cpu: 20m
  ruler:
    resources:
      requests:
        cpu: 20m
  store_gateway:
    zoneAwareReplication:
      enabled: false
    resources:
      requests:
        cpu: 20m
  minio:
    resources:
      requests:
        cpu: 20m
  rollout_operator:
    resources:
      requests:
        cpu: 20m

tempo:
  # -- Deploy Tempo if enabled.  See [upstream readme](https://github.com/grafana/helm-charts/blob/main/charts/tempo-distributed/README.md#values) for full values reference.
  enabled: true
  serviceAccount:
    create: false
    name: lgtm
  # Pin Tempo image to match subchart defaults (used by all core components)
  image:
    registry: docker.io
    repository: grafana/tempo
    tag: "2.8.0"
  # Components to run
  ingester:
    replicas: 1
    config:
      # Single-ingester POC: write without replication
      replication_factor: 1
    resources:
      requests:
        cpu: 50m
        memory: 128Mi
      limits:
        cpu: 200m
        memory: 128Mi
  distributor:
    replicas: 1
    resources:
      requests:
        cpu: 50m
        memory: 128Mi
      limits:
        cpu: 200m
        memory: 128Mi
  querier:
    replicas: 1
    resources:
      requests:
        cpu: 50m
        memory: 128Mi
      limits:
        cpu: 200m
        memory: 128Mi
  queryFrontend:
    replicas: 1
    resources:
      requests:
        cpu: 50m
        memory: 128Mi
      limits:
        cpu: 200m
        memory: 128Mi
  # Enable OTLP ingestion on distributor (gRPC/HTTP)
  traces:
    otlp:
      grpc:
        enabled: true
      http:
        enabled: true
  # Disable memcached and clear caches
  memcached:
    enabled: false
  cache:
    caches: []

grafana-oncall:
  # -- Deploy Grafana OnCall if enabled. See [upstream values.yaml](https://github.com/grafana/oncall/blob/dev/helm/oncall/values.yaml) for full values reference.
  enabled: false

# OpenTelemetry Collector subchart values
otelcol:
  enabled: true
  serviceAccount:
    create: false
    name: lgtm
  mode: deployment
  image:
    repository: ghcr.io/open-telemetry/opentelemetry-collector-releases/opentelemetry-collector-k8s
    # tag left empty to use chart appVersion (0.134.1)
    tag: ""
  command:
    name: otelcol-k8s
  # Minimal config to accept OTLP and export traces to Tempo distributor
  config:
    exporters:
      otlphttp:
        endpoint: "http://{{ .Release.Name }}-tempo-distributor:4318"
        tls:
          insecure: true
    service:
      pipelines:
        traces:
          exporters:
            - otlphttp
            - debug
  # Export the Collector's own traces to Tempo via OTLP/HTTP
  internalTelemetryViaOTLP:
    traces:
      enabled: true
      endpoint: "http://{{ .Release.Name }}-tempo-distributor:4318"
  # Give self-telemetry a friendly service name
  extraEnvs:
    - name: OTEL_SERVICE_NAME
      value: "{{ .Release.Name }}-otelcol"
  # Small footprint resources for OpenTelemetry Collector
  resources:
    requests:
      cpu: 50m
      memory: 128Mi
    limits:
      cpu: 200m
      memory: 128Mi
