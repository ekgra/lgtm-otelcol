---
# Source: lgtm-distributed/charts/loki/charts/minio/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: "minio-sa"
  namespace: "lgtm-poc-8"
---
# Source: lgtm-distributed/charts/loki/charts/minio/templates/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: lgtm-minio
  namespace: "lgtm-poc-8"
  labels:
    app: minio
    chart: minio-4.0.15
    release: lgtm
    heritage: Helm
type: Opaque
data:
  rootUser: "ZW50ZXJwcmlzZS1sb2dz"
  rootPassword: "c3VwZXJzZWNyZXQ="
---
# Source: lgtm-distributed/charts/loki/charts/minio/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: lgtm-minio
  namespace: "lgtm-poc-8"
  labels:
    app: minio
    chart: minio-4.0.15
    release: lgtm
    heritage: Helm
data:
  initialize: |-
    #!/bin/sh
    set -e ; # Have script exit in the event of a failed command.
    MC_CONFIG_DIR="/etc/minio/mc/"
    MC="/usr/bin/mc --insecure --config-dir ${MC_CONFIG_DIR}"
    
    # connectToMinio
    # Use a check-sleep-check loop to wait for MinIO service to be available
    connectToMinio() {
      SCHEME=$1
      ATTEMPTS=0 ; LIMIT=29 ; # Allow 30 attempts
      set -e ; # fail if we can't read the keys.
      ACCESS=$(cat /config/rootUser) ; SECRET=$(cat /config/rootPassword) ;
      set +e ; # The connections to minio are allowed to fail.
      echo "Connecting to MinIO server: $SCHEME://$MINIO_ENDPOINT:$MINIO_PORT" ;
      MC_COMMAND="${MC} alias set myminio $SCHEME://$MINIO_ENDPOINT:$MINIO_PORT $ACCESS $SECRET" ;
      $MC_COMMAND ;
      STATUS=$? ;
      until [ $STATUS = 0 ]
      do
        ATTEMPTS=`expr $ATTEMPTS + 1` ;
        echo \"Failed attempts: $ATTEMPTS\" ;
        if [ $ATTEMPTS -gt $LIMIT ]; then
          exit 1 ;
        fi ;
        sleep 2 ; # 1 second intervals between attempts
        $MC_COMMAND ;
        STATUS=$? ;
      done ;
      set -e ; # reset `e` as active
      return 0
    }
    
    # checkBucketExists ($bucket)
    # Check if the bucket exists, by using the exit code of `mc ls`
    checkBucketExists() {
      BUCKET=$1
      CMD=$(${MC} ls myminio/$BUCKET > /dev/null 2>&1)
      return $?
    }
    
    # createBucket ($bucket, $policy, $purge)
    # Ensure bucket exists, purging if asked to
    createBucket() {
      BUCKET=$1
      POLICY=$2
      PURGE=$3
      VERSIONING=$4
      OBJECTLOCKING=$5
    
      # Purge the bucket, if set & exists
      # Since PURGE is user input, check explicitly for `true`
      if [ $PURGE = true ]; then
        if checkBucketExists $BUCKET ; then
          echo "Purging bucket '$BUCKET'."
          set +e ; # don't exit if this fails
          ${MC} rm -r --force myminio/$BUCKET
          set -e ; # reset `e` as active
        else
          echo "Bucket '$BUCKET' does not exist, skipping purge."
        fi
      fi
    
    # Create the bucket if it does not exist and set objectlocking if enabled (NOTE: versioning will be not changed if OBJECTLOCKING is set because it enables versioning to the Buckets created)
    if ! checkBucketExists $BUCKET ; then
        if [ ! -z $OBJECTLOCKING ] ; then
          if [ $OBJECTLOCKING = true ] ; then
              echo "Creating bucket with OBJECTLOCKING '$BUCKET'"
              ${MC} mb --with-lock myminio/$BUCKET
          elif [ $OBJECTLOCKING = false ] ; then
                echo "Creating bucket '$BUCKET'"
                ${MC} mb myminio/$BUCKET
          fi
      elif [ -z $OBJECTLOCKING ] ; then
            echo "Creating bucket '$BUCKET'"
            ${MC} mb myminio/$BUCKET
      else
        echo "Bucket '$BUCKET' already exists."  
      fi
      fi
    
    
      # set versioning for bucket if objectlocking is disabled or not set
      if [ -z $OBJECTLOCKING ] ; then
      if [ ! -z $VERSIONING ] ; then
        if [ $VERSIONING = true ] ; then
            echo "Enabling versioning for '$BUCKET'"
            ${MC} version enable myminio/$BUCKET
        elif [ $VERSIONING = false ] ; then
            echo "Suspending versioning for '$BUCKET'"
            ${MC} version suspend myminio/$BUCKET
        fi
        fi
      else
          echo "Bucket '$BUCKET' versioning unchanged."
      fi
    
    
      # At this point, the bucket should exist, skip checking for existence
      # Set policy on the bucket
      echo "Setting policy of bucket '$BUCKET' to '$POLICY'."
      ${MC} policy set $POLICY myminio/$BUCKET
    }
    
    # Try connecting to MinIO instance
    scheme=http
    connectToMinio $scheme
    
    
    
    # Create the buckets
    createBucket chunks none false  
    createBucket ruler none false  
    createBucket admin none false  
  add-user: |-
    #!/bin/sh
    set -e ; # Have script exit in the event of a failed command.
    MC_CONFIG_DIR="/etc/minio/mc/"
    MC="/usr/bin/mc --insecure --config-dir ${MC_CONFIG_DIR}"
    
    # AccessKey and secretkey credentials file are added to prevent shell execution errors caused by special characters.
    # Special characters for example : ',",<,>,{,}
    MINIO_ACCESSKEY_SECRETKEY_TMP="/tmp/accessKey_and_secretKey_tmp"
    
    # connectToMinio
    # Use a check-sleep-check loop to wait for MinIO service to be available
    connectToMinio() {
      SCHEME=$1
      ATTEMPTS=0 ; LIMIT=29 ; # Allow 30 attempts
      set -e ; # fail if we can't read the keys.
      ACCESS=$(cat /config/rootUser) ; SECRET=$(cat /config/rootPassword) ;
      set +e ; # The connections to minio are allowed to fail.
      echo "Connecting to MinIO server: $SCHEME://$MINIO_ENDPOINT:$MINIO_PORT" ;
      MC_COMMAND="${MC} alias set myminio $SCHEME://$MINIO_ENDPOINT:$MINIO_PORT $ACCESS $SECRET" ;
      $MC_COMMAND ;
      STATUS=$? ;
      until [ $STATUS = 0 ]
      do
        ATTEMPTS=`expr $ATTEMPTS + 1` ;
        echo \"Failed attempts: $ATTEMPTS\" ;
        if [ $ATTEMPTS -gt $LIMIT ]; then
          exit 1 ;
        fi ;
        sleep 2 ; # 1 second intervals between attempts
        $MC_COMMAND ;
        STATUS=$? ;
      done ;
      set -e ; # reset `e` as active
      return 0
    }
    
    # checkUserExists ()
    # Check if the user exists, by using the exit code of `mc admin user info`
    checkUserExists() {
      CMD=$(${MC} admin user info myminio $(head -1 $MINIO_ACCESSKEY_SECRETKEY_TMP) > /dev/null 2>&1)
      return $?
    }
    
    # createUser ($policy)
    createUser() {
      POLICY=$1
      #check accessKey_and_secretKey_tmp file
      if [[ ! -f $MINIO_ACCESSKEY_SECRETKEY_TMP ]];then
        echo "credentials file does not exist"
        return 1
      fi
      if [[ $(cat $MINIO_ACCESSKEY_SECRETKEY_TMP|wc -l) -ne 2 ]];then
        echo "credentials file is invalid"
        rm -f $MINIO_ACCESSKEY_SECRETKEY_TMP
        return 1
      fi
      USER=$(head -1 $MINIO_ACCESSKEY_SECRETKEY_TMP)
      # Create the user if it does not exist
      if ! checkUserExists ; then
        echo "Creating user '$USER'"
        cat $MINIO_ACCESSKEY_SECRETKEY_TMP | ${MC} admin user add myminio
      else
        echo "User '$USER' already exists."
      fi
      #clean up credentials files.
      rm -f $MINIO_ACCESSKEY_SECRETKEY_TMP
    
      # set policy for user
      if [ ! -z $POLICY -a $POLICY != " " ] ; then
          echo "Adding policy '$POLICY' for '$USER'"
          ${MC} admin policy set myminio $POLICY user=$USER
      else
          echo "User '$USER' has no policy attached."
      fi
    }
    
    # Try connecting to MinIO instance
    scheme=http
    connectToMinio $scheme
    
    
    
    # Create the users
    echo console > $MINIO_ACCESSKEY_SECRETKEY_TMP
    echo console123 >> $MINIO_ACCESSKEY_SECRETKEY_TMP
    createUser consoleAdmin
    
  add-policy: |-
    #!/bin/sh
    set -e ; # Have script exit in the event of a failed command.
    MC_CONFIG_DIR="/etc/minio/mc/"
    MC="/usr/bin/mc --insecure --config-dir ${MC_CONFIG_DIR}"
    
    # connectToMinio
    # Use a check-sleep-check loop to wait for MinIO service to be available
    connectToMinio() {
      SCHEME=$1
      ATTEMPTS=0 ; LIMIT=29 ; # Allow 30 attempts
      set -e ; # fail if we can't read the keys.
      ACCESS=$(cat /config/rootUser) ; SECRET=$(cat /config/rootPassword) ;
      set +e ; # The connections to minio are allowed to fail.
      echo "Connecting to MinIO server: $SCHEME://$MINIO_ENDPOINT:$MINIO_PORT" ;
      MC_COMMAND="${MC} alias set myminio $SCHEME://$MINIO_ENDPOINT:$MINIO_PORT $ACCESS $SECRET" ;
      $MC_COMMAND ;
      STATUS=$? ;
      until [ $STATUS = 0 ]
      do
        ATTEMPTS=`expr $ATTEMPTS + 1` ;
        echo \"Failed attempts: $ATTEMPTS\" ;
        if [ $ATTEMPTS -gt $LIMIT ]; then
          exit 1 ;
        fi ;
        sleep 2 ; # 1 second intervals between attempts
        $MC_COMMAND ;
        STATUS=$? ;
      done ;
      set -e ; # reset `e` as active
      return 0
    }
    
    # checkPolicyExists ($policy)
    # Check if the policy exists, by using the exit code of `mc admin policy info`
    checkPolicyExists() {
      POLICY=$1
      CMD=$(${MC} admin policy info myminio $POLICY > /dev/null 2>&1)
      return $?
    }
    
    # createPolicy($name, $filename)
    createPolicy () {
      NAME=$1
      FILENAME=$2
    
      # Create the name if it does not exist
      echo "Checking policy: $NAME (in /config/$FILENAME.json)"
      if ! checkPolicyExists $NAME ; then
        echo "Creating policy '$NAME'"
      else
        echo "Policy '$NAME' already exists."
      fi
      ${MC} admin policy add myminio $NAME /config/$FILENAME.json
    
    }
    
    # Try connecting to MinIO instance
    scheme=http
    connectToMinio $scheme
    
    
  custom-command: |-
    #!/bin/sh
    set -e ; # Have script exit in the event of a failed command.
    MC_CONFIG_DIR="/etc/minio/mc/"
    MC="/usr/bin/mc --insecure --config-dir ${MC_CONFIG_DIR}"
    
    # connectToMinio
    # Use a check-sleep-check loop to wait for MinIO service to be available
    connectToMinio() {
      SCHEME=$1
      ATTEMPTS=0 ; LIMIT=29 ; # Allow 30 attempts
      set -e ; # fail if we can't read the keys.
      ACCESS=$(cat /config/rootUser) ; SECRET=$(cat /config/rootPassword) ;
      set +e ; # The connections to minio are allowed to fail.
      echo "Connecting to MinIO server: $SCHEME://$MINIO_ENDPOINT:$MINIO_PORT" ;
      MC_COMMAND="${MC} alias set myminio $SCHEME://$MINIO_ENDPOINT:$MINIO_PORT $ACCESS $SECRET" ;
      $MC_COMMAND ;
      STATUS=$? ;
      until [ $STATUS = 0 ]
      do
        ATTEMPTS=`expr $ATTEMPTS + 1` ;
        echo \"Failed attempts: $ATTEMPTS\" ;
        if [ $ATTEMPTS -gt $LIMIT ]; then
          exit 1 ;
        fi ;
        sleep 2 ; # 1 second intervals between attempts
        $MC_COMMAND ;
        STATUS=$? ;
      done ;
      set -e ; # reset `e` as active
      return 0
    }
    
    # runCommand ($@)
    # Run custom mc command
    runCommand() {
      ${MC} "$@"
      return $?
    }
    
    # Try connecting to MinIO instance
    scheme=http
    connectToMinio $scheme
---
# Source: lgtm-distributed/charts/loki/templates/config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: loki
  namespace: lgtm-poc-8
  labels:
    helm.sh/chart: loki-6.9.0
    app.kubernetes.io/name: loki
    app.kubernetes.io/instance: lgtm
    app.kubernetes.io/version: "3.1.0"
    app.kubernetes.io/managed-by: Helm
data:
  config.yaml: |
    
    auth_enabled: false
    common:
      compactor_address: 'http://lgtm-loki-compactor:3100'
      path_prefix: /var/loki
      replication_factor: 1
      storage:
        s3:
          access_key_id: enterprise-logs
          bucketnames: chunks
          endpoint: lgtm-minio.lgtm-poc-8.svc:9000
          insecure: true
          s3forcepathstyle: true
          secret_access_key: supersecret
    frontend:
      scheduler_address: lgtm-loki-query-scheduler.lgtm-poc-8.svc.cluster.local:9095
      tail_proxy_url: http://lgtm-loki-querier.lgtm-poc-8.svc.cluster.local:3100
    frontend_worker:
      scheduler_address: lgtm-loki-query-scheduler.lgtm-poc-8.svc.cluster.local:9095
    index_gateway:
      mode: simple
    limits_config:
      max_cache_freshness_per_query: 10m
      query_timeout: 300s
      reject_old_samples: true
      reject_old_samples_max_age: 168h
      split_queries_by_interval: 15m
      volume_enabled: true
    memberlist:
      join_members:
      - loki-memberlist
    pattern_ingester:
      enabled: false
    query_range:
      align_queries_with_step: true
    ruler:
      storage:
        s3:
          bucketnames: ruler
        type: s3
    runtime_config:
      file: /etc/loki/runtime-config/runtime-config.yaml
    schema_config:
      configs:
      - from: "2024-01-01"
        index:
          period: 24h
          prefix: index_
        object_store: s3
        schema: v13
        store: tsdb
    server:
      grpc_listen_port: 9095
      http_listen_port: 3100
      http_server_read_timeout: 600s
      http_server_write_timeout: 600s
    storage_config:
      boltdb_shipper:
        index_gateway_client:
          server_address: dns+lgtm-loki-index-gateway-headless.lgtm-poc-8.svc.cluster.local:9095
      hedging:
        at: 250ms
        max_per_second: 20
        up_to: 3
      tsdb_shipper:
        index_gateway_client:
          server_address: dns+lgtm-loki-index-gateway-headless.lgtm-poc-8.svc.cluster.local:9095
    tracing:
      enabled: false
---
# Source: lgtm-distributed/charts/loki/templates/runtime-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: loki-runtime
  namespace: lgtm-poc-8
  labels:
    helm.sh/chart: loki-6.9.0
    app.kubernetes.io/name: loki
    app.kubernetes.io/instance: lgtm
    app.kubernetes.io/version: "3.1.0"
    app.kubernetes.io/managed-by: Helm
data:
  runtime-config.yaml: |
    {}
---
# Source: lgtm-distributed/charts/loki/charts/minio/templates/console-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: lgtm-minio-console
  namespace: "lgtm-poc-8"
  labels:
    app: minio
    chart: minio-4.0.15
    release: lgtm
    heritage: Helm
spec:
  type: ClusterIP
  ports:
    - name: http
      port: 9001
      protocol: TCP
      targetPort: 9001
  selector:
    app: minio
    release: lgtm
---
# Source: lgtm-distributed/charts/loki/charts/minio/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: lgtm-minio
  namespace: "lgtm-poc-8"
  labels:
    app: minio
    chart: minio-4.0.15
    release: lgtm
    heritage: Helm
    monitoring: "true"
spec:
  type: ClusterIP
  ports:
    - name: http
      port: 9000
      protocol: TCP
      targetPort: 9000
  selector:
    app: minio
    release: lgtm
---
# Source: lgtm-distributed/charts/loki/charts/minio/templates/statefulset.yaml
apiVersion: v1
kind: Service
metadata:
  name: lgtm-minio-svc
  namespace: "lgtm-poc-8"
  labels:
    app: minio
    chart: minio-4.0.15
    release: "lgtm"
    heritage: "Helm"
spec:
  publishNotReadyAddresses: true
  clusterIP: None
  ports:
    - name: http
      port: 9000
      protocol: TCP
      targetPort: 9000
  selector:
    app: minio
    release: lgtm
---
# Source: lgtm-distributed/charts/loki/templates/compactor/service-compactor.yaml
apiVersion: v1
kind: Service
metadata:
  name: lgtm-loki-compactor
  namespace: lgtm-poc-8
  labels:
    helm.sh/chart: loki-6.9.0
    app.kubernetes.io/name: loki
    app.kubernetes.io/instance: lgtm
    app.kubernetes.io/version: "3.1.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: compactor
spec:
  type: ClusterIP
  ports:
    - name: http-metrics
      port: 3100
      targetPort: http-metrics
      protocol: TCP
    - name: grpc
      port: 9095
      targetPort: grpc
      protocol: TCP
  selector:
    app.kubernetes.io/name: loki
    app.kubernetes.io/instance: lgtm
    app.kubernetes.io/component: compactor
---
# Source: lgtm-distributed/charts/loki/templates/distributor/service-distributor-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: lgtm-loki-distributor-headless
  namespace: lgtm-poc-8
  labels:
    app.kubernetes.io/name: loki
    app.kubernetes.io/instance: lgtm
    app.kubernetes.io/component: distributor
    variant: headless
    prometheus.io/service-monitor: "false"
spec:
  type: ClusterIP
  clusterIP: None
  ports:
    - name: http-metrics
      port: 3100
      targetPort: http-metrics
      protocol: TCP
    - name: grpc
      port: 9095
      targetPort: grpc
      protocol: TCP
  selector:
    app.kubernetes.io/name: loki
    app.kubernetes.io/instance: lgtm
    app.kubernetes.io/component: distributor
---
# Source: lgtm-distributed/charts/loki/templates/distributor/service-distributor.yaml
apiVersion: v1
kind: Service
metadata:
  name: lgtm-loki-distributor
  namespace: lgtm-poc-8
  labels:
    helm.sh/chart: loki-6.9.0
    app.kubernetes.io/name: loki
    app.kubernetes.io/instance: lgtm
    app.kubernetes.io/version: "3.1.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: distributor
spec:
  type: ClusterIP
  ports:
    - name: http-metrics
      port: 3100
      targetPort: http-metrics
      protocol: TCP
    - name: grpc
      port: 9095
      targetPort: grpc
      protocol: TCP
  selector:
    app.kubernetes.io/name: loki
    app.kubernetes.io/instance: lgtm
    app.kubernetes.io/component: distributor
---
# Source: lgtm-distributed/charts/loki/templates/index-gateway/service-index-gateway-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: lgtm-loki-index-gateway-headless
  labels:
    app.kubernetes.io/name: loki
    app.kubernetes.io/instance: lgtm
    app.kubernetes.io/component: index-gateway
    prometheus.io/service-monitor: "false"
spec:
  type: ClusterIP
  clusterIP: None
  ports:
    - name: http-metrics
      port: 3100
      targetPort: http-metrics
      protocol: TCP
    - name: grpc
      port: 9095
      targetPort: grpc
      protocol: TCP
  selector:
    app.kubernetes.io/name: loki
    app.kubernetes.io/instance: lgtm
    app.kubernetes.io/component: index-gateway
---
# Source: lgtm-distributed/charts/loki/templates/index-gateway/service-index-gateway.yaml
apiVersion: v1
kind: Service
metadata:
  name: lgtm-loki-index-gateway
  labels:
    helm.sh/chart: loki-6.9.0
    app.kubernetes.io/name: loki
    app.kubernetes.io/instance: lgtm
    app.kubernetes.io/version: "3.1.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: index-gateway
spec:
  type: ClusterIP
  ports:
    - name: http-metrics
      port: 3100
      targetPort: http-metrics
      protocol: TCP
    - name: grpc
      port: 9095
      targetPort: grpc
      protocol: TCP
  selector:
    app.kubernetes.io/name: loki
    app.kubernetes.io/instance: lgtm
    app.kubernetes.io/component: index-gateway
---
# Source: lgtm-distributed/charts/loki/templates/ingester/service-ingester-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: lgtm-loki-ingester-headless
  namespace: lgtm-poc-8
  labels:
    app.kubernetes.io/name: loki
    app.kubernetes.io/instance: lgtm
    app.kubernetes.io/component: ingester
    prometheus.io/service-monitor: "false"
spec:
  type: ClusterIP
  clusterIP: None
  ports:
    - name: http-metrics
      port: 3100
      targetPort: http-metrics
      protocol: TCP
    - name: grpc
      port: 9095
      targetPort: grpc
      protocol: TCP
  selector:
    app.kubernetes.io/name: loki
    app.kubernetes.io/instance: lgtm
    app.kubernetes.io/component: ingester
---
# Source: lgtm-distributed/charts/loki/templates/ingester/service-ingester.yaml
apiVersion: v1
kind: Service
metadata:
  name: lgtm-loki-ingester
  namespace: lgtm-poc-8
  labels:
    helm.sh/chart: loki-6.9.0
    app.kubernetes.io/name: loki
    app.kubernetes.io/instance: lgtm
    app.kubernetes.io/version: "3.1.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: ingester
spec:
  type: ClusterIP
  ports:
    - name: http-metrics
      port: 3100
      targetPort: http-metrics
      protocol: TCP
    - name: grpc
      port: 9095
      targetPort: grpc
      protocol: TCP
  selector:
    app.kubernetes.io/name: loki
    app.kubernetes.io/instance: lgtm
    app.kubernetes.io/component: ingester
---
# Source: lgtm-distributed/charts/loki/templates/querier/service-querier.yaml
apiVersion: v1
kind: Service
metadata:
  name: lgtm-loki-querier
  namespace: lgtm-poc-8
  labels:
    helm.sh/chart: loki-6.9.0
    app.kubernetes.io/name: loki
    app.kubernetes.io/instance: lgtm
    app.kubernetes.io/version: "3.1.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: querier
spec:
  type: ClusterIP
  ports:
    - name: http-metrics
      port: 3100
      targetPort: http-metrics
      protocol: TCP
    - name: grpc
      port: 9095
      targetPort: grpc
      protocol: TCP
  selector:
    app.kubernetes.io/name: loki
    app.kubernetes.io/instance: lgtm
    app.kubernetes.io/component: querier
---
# Source: lgtm-distributed/charts/loki/templates/query-frontend/service-query-frontend-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: lgtm-loki-query-frontend-headless
  namespace: lgtm-poc-8
  labels:
    helm.sh/chart: loki-6.9.0
    app.kubernetes.io/name: loki
    app.kubernetes.io/instance: lgtm
    app.kubernetes.io/version: "3.1.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: query-frontend
    prometheus.io/service-monitor: "false"
spec:
  clusterIP: None
  type: ClusterIP
  publishNotReadyAddresses: true
  ports:
    - name: http-metrics
      port: 3100
      targetPort: http-metrics
      protocol: TCP
    - name: grpc
      port: 9095
      targetPort: grpc
      protocol: TCP
    - name: grpclb
      port: 9096
      targetPort: grpc
      protocol: TCP
  selector:
    app.kubernetes.io/name: loki
    app.kubernetes.io/instance: lgtm
    app.kubernetes.io/component: query-frontend
---
# Source: lgtm-distributed/charts/loki/templates/query-frontend/service-query-frontend.yaml
apiVersion: v1
kind: Service
metadata:
  name: lgtm-loki-query-frontend
  namespace: lgtm-poc-8
  labels:
    helm.sh/chart: loki-6.9.0
    app.kubernetes.io/name: loki
    app.kubernetes.io/instance: lgtm
    app.kubernetes.io/version: "3.1.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: query-frontend
spec:
  type: ClusterIP
  publishNotReadyAddresses: true
  ports:
    - name: http-metrics
      port: 3100
      targetPort: http-metrics
      protocol: TCP
    - name: grpc
      port: 9095
      targetPort: grpc
      protocol: TCP
    - name: grpclb
      port: 9096
      targetPort: grpc
      protocol: TCP
  selector:
    app.kubernetes.io/name: loki
    app.kubernetes.io/instance: lgtm
    app.kubernetes.io/component: query-frontend
---
# Source: lgtm-distributed/charts/loki/templates/query-scheduler/service-query-scheduler.yaml
apiVersion: v1
kind: Service
metadata:
  name: lgtm-loki-query-scheduler
  namespace: lgtm-poc-8
  labels:
    helm.sh/chart: loki-6.9.0
    app.kubernetes.io/name: loki
    app.kubernetes.io/instance: lgtm
    app.kubernetes.io/version: "3.1.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: query-scheduler
spec:
  type: ClusterIP
  clusterIP: None
  publishNotReadyAddresses: true
  ports:
    - name: http-metrics
      port: 3100
      targetPort: http-metrics
      protocol: TCP
    - name: grpclb
      port: 9095
      targetPort: grpc
      protocol: TCP
  selector:
    app.kubernetes.io/name: loki
    app.kubernetes.io/instance: lgtm
    app.kubernetes.io/component: query-scheduler
---
# Source: lgtm-distributed/charts/loki/templates/service-memberlist.yaml
apiVersion: v1
kind: Service
metadata:
  name: loki-memberlist
  namespace: lgtm-poc-8
  labels:
    helm.sh/chart: loki-6.9.0
    app.kubernetes.io/name: loki
    app.kubernetes.io/instance: lgtm
    app.kubernetes.io/version: "3.1.0"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  clusterIP: None
  ports:
    - name: tcp
      port: 7946
      targetPort: http-memberlist
      protocol: TCP
  selector:
    app.kubernetes.io/name: loki
    app.kubernetes.io/instance: lgtm
    app.kubernetes.io/part-of: memberlist
---
# Source: lgtm-distributed/charts/loki/templates/distributor/deployment-distributor.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: lgtm-loki-distributor
  namespace: lgtm-poc-8
  labels:
    helm.sh/chart: loki-6.9.0
    app.kubernetes.io/name: loki
    app.kubernetes.io/instance: lgtm
    app.kubernetes.io/version: "3.1.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: distributor
    app.kubernetes.io/part-of: memberlist
spec:
  replicas: 1
  strategy:
    rollingUpdate:
      maxSurge: 0
      maxUnavailable: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app.kubernetes.io/name: loki
      app.kubernetes.io/instance: lgtm
      app.kubernetes.io/component: distributor
  template:
    metadata:
      annotations:
        checksum/config: 2a79ed5c83e3dc3330ad5e8e316cc0319e73df426335c55911206c359308d362
      labels:
        app.kubernetes.io/name: loki
        app.kubernetes.io/instance: lgtm
        app.kubernetes.io/component: distributor
        app.kubernetes.io/part-of: memberlist
    spec:
      serviceAccountName: lgtm
      
      securityContext:
        fsGroup: 10001
        runAsGroup: 10001
        runAsNonRoot: true
        runAsUser: 10001
      terminationGracePeriodSeconds: 30
      containers:
        - name: distributor
          image: docker.io/grafana/loki:3.1.0
          imagePullPolicy: IfNotPresent
          args:
            - -config.file=/etc/loki/config/config.yaml
            - -target=distributor
          ports:
            - name: http-metrics
              containerPort: 3100
              protocol: TCP
            - name: grpc
              containerPort: 9095
              protocol: TCP
            - name: http-memberlist
              containerPort: 7946
              protocol: TCP
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          readinessProbe:
            httpGet:
              path: /ready
              port: http-metrics
            initialDelaySeconds: 30
            timeoutSeconds: 1
          livenessProbe:
            null
          volumeMounts:
            - name: config
              mountPath: /etc/loki/config
            - name: runtime-config
              mountPath: /etc/loki/runtime-config
          resources:
            limits:
              cpu: 200m
              memory: 128Mi
            requests:
              cpu: 50m
              memory: 128Mi
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                app.kubernetes.io/component: distributor
            topologyKey: kubernetes.io/hostname
      volumes:
        - name: config
          configMap:
            name: loki
            items:
              - key: "config.yaml"
                path: "config.yaml"
        - name: runtime-config
          configMap:
            name: loki-runtime
---
# Source: lgtm-distributed/charts/loki/templates/querier/deployment-querier.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: lgtm-loki-querier
  namespace: lgtm-poc-8
  labels:
    helm.sh/chart: loki-6.9.0
    app.kubernetes.io/name: loki
    app.kubernetes.io/instance: lgtm
    app.kubernetes.io/version: "3.1.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: querier
    app.kubernetes.io/part-of: memberlist
spec:
  replicas: 1
  strategy:
    rollingUpdate:
      maxSurge: 0
      maxUnavailable: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app.kubernetes.io/name: loki
      app.kubernetes.io/instance: lgtm
      app.kubernetes.io/component: querier
  template:
    metadata:
      annotations:
        checksum/config: 2a79ed5c83e3dc3330ad5e8e316cc0319e73df426335c55911206c359308d362
      labels:
        app.kubernetes.io/name: loki
        app.kubernetes.io/instance: lgtm
        app.kubernetes.io/component: querier
        app.kubernetes.io/part-of: memberlist
    spec:
      topologySpreadConstraints:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/component: querier
          maxSkew: 1
          topologyKey: kubernetes.io/hostname
          whenUnsatisfiable: ScheduleAnyway
      serviceAccountName: lgtm
      
      securityContext:
        fsGroup: 10001
        runAsGroup: 10001
        runAsNonRoot: true
        runAsUser: 10001
      terminationGracePeriodSeconds: 30
      containers:
        - name: querier
          image: docker.io/grafana/loki:3.1.0
          imagePullPolicy: IfNotPresent
          args:
            - -config.file=/etc/loki/config/config.yaml
            - -target=querier
          ports:
            - name: http-metrics
              containerPort: 3100
              protocol: TCP
            - name: grpc
              containerPort: 9095
              protocol: TCP
            - name: http-memberlist
              containerPort: 7946
              protocol: TCP
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          readinessProbe:
            httpGet:
              path: /ready
              port: http-metrics
            initialDelaySeconds: 30
            timeoutSeconds: 1
          livenessProbe:
            null
          volumeMounts:
            - name: config
              mountPath: /etc/loki/config
            - name: runtime-config
              mountPath: /etc/loki/runtime-config
            - name: data
              mountPath: /var/loki
          resources:
            limits:
              cpu: 200m
              memory: 128Mi
            requests:
              cpu: 50m
              memory: 128Mi
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                app.kubernetes.io/component: querier
            topologyKey: kubernetes.io/hostname
      volumes:
        - name: config
          configMap:
            name: loki
            items:
              - key: "config.yaml"
                path: "config.yaml"
        - name: runtime-config
          configMap:
            name: loki-runtime
        - name: data
          emptyDir: {}
---
# Source: lgtm-distributed/charts/loki/templates/query-frontend/deployment-query-frontend.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: lgtm-loki-query-frontend
  namespace: lgtm-poc-8
  labels:
    helm.sh/chart: loki-6.9.0
    app.kubernetes.io/name: loki
    app.kubernetes.io/instance: lgtm
    app.kubernetes.io/version: "3.1.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: query-frontend
spec:
  replicas: 1
  strategy:
    rollingUpdate:
      maxSurge: 0
      maxUnavailable: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app.kubernetes.io/name: loki
      app.kubernetes.io/instance: lgtm
      app.kubernetes.io/component: query-frontend
  template:
    metadata:
      annotations:
        checksum/config: 2a79ed5c83e3dc3330ad5e8e316cc0319e73df426335c55911206c359308d362
      labels:
        app.kubernetes.io/name: loki
        app.kubernetes.io/instance: lgtm
        app.kubernetes.io/component: query-frontend
        app.kubernetes.io/part-of: memberlist
    spec:
      serviceAccountName: lgtm
      
      securityContext:
        fsGroup: 10001
        runAsGroup: 10001
        runAsNonRoot: true
        runAsUser: 10001
      terminationGracePeriodSeconds: 30
      containers:
        - name: query-frontend
          image: docker.io/grafana/loki:3.1.0
          imagePullPolicy: IfNotPresent
          args:
            - -config.file=/etc/loki/config/config.yaml
            - -target=query-frontend
          ports:
            - name: http-metrics
              containerPort: 3100
              protocol: TCP
            - name: grpc
              containerPort: 9095
              protocol: TCP
            - name: http-memberlist
              containerPort: 7946
              protocol: TCP
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          readinessProbe:
            httpGet:
              path: /ready
              port: http-metrics
            initialDelaySeconds: 30
            timeoutSeconds: 1
          volumeMounts:
            - name: config
              mountPath: /etc/loki/config
            - name: runtime-config
              mountPath: /etc/loki/runtime-config
          resources:
            limits:
              cpu: 200m
              memory: 128Mi
            requests:
              cpu: 50m
              memory: 128Mi
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                app.kubernetes.io/component: query-frontend
            topologyKey: kubernetes.io/hostname
      volumes:
        - name: config
          configMap:
            name: loki
            items:
              - key: "config.yaml"
                path: "config.yaml"
        - name: runtime-config
          configMap:
            name: loki-runtime
---
# Source: lgtm-distributed/charts/loki/templates/query-scheduler/deployment-query-scheduler.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: lgtm-loki-query-scheduler
  namespace: lgtm-poc-8
  labels:
    helm.sh/chart: loki-6.9.0
    app.kubernetes.io/name: loki
    app.kubernetes.io/instance: lgtm
    app.kubernetes.io/version: "3.1.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: query-scheduler
spec:
  replicas: 1
  strategy:
    rollingUpdate:
      maxSurge: 0
      maxUnavailable: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app.kubernetes.io/name: loki
      app.kubernetes.io/instance: lgtm
      app.kubernetes.io/component: query-scheduler
  template:
    metadata:
      annotations:
        checksum/config: 2a79ed5c83e3dc3330ad5e8e316cc0319e73df426335c55911206c359308d362
      labels:
        app.kubernetes.io/name: loki
        app.kubernetes.io/instance: lgtm
        app.kubernetes.io/component: query-scheduler
        app.kubernetes.io/part-of: memberlist
    spec:
      serviceAccountName: lgtm
      
      securityContext:
        fsGroup: 10001
        runAsGroup: 10001
        runAsNonRoot: true
        runAsUser: 10001
      terminationGracePeriodSeconds: 30
      containers:
        - name: query-scheduler
          image: docker.io/grafana/loki:3.1.0
          imagePullPolicy: IfNotPresent
          args:
            - -config.file=/etc/loki/config/config.yaml
            - -target=query-scheduler
          ports:
            - name: http-metrics
              containerPort: 3100
              protocol: TCP
            - name: grpc
              containerPort: 9095
              protocol: TCP
            - name: http-memberlist
              containerPort: 7946
              protocol: TCP
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          readinessProbe:
            httpGet:
              path: /ready
              port: http-metrics
            initialDelaySeconds: 30
            timeoutSeconds: 1
          livenessProbe:
            null
          volumeMounts:
            - name: config
              mountPath: /etc/loki/config
            - name: runtime-config
              mountPath: /etc/loki/runtime-config
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                app.kubernetes.io/component: query-scheduler
            topologyKey: kubernetes.io/hostname
      volumes:
        - name: config
          configMap:
            name: loki
            items:
              - key: "config.yaml"
                path: "config.yaml"
        - name: runtime-config
          configMap:
            name: loki-runtime
---
# Source: lgtm-distributed/charts/loki/charts/minio/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: lgtm-minio
  namespace: "lgtm-poc-8"
  labels:
    app: minio
    chart: minio-4.0.15
    release: lgtm
    heritage: Helm
spec:
  updateStrategy:
    type: RollingUpdate
  podManagementPolicy: "Parallel"
  serviceName: lgtm-minio-svc
  replicas: 1
  selector:
    matchLabels:
      app: minio
      release: lgtm
  template:
    metadata:
      name: lgtm-minio
      labels:
        app: minio
        release: lgtm
      annotations:
        checksum/secrets: bb6eb18776deb7b7c400f3b203d77edf18c7c49c7596b3686fa9096f04ca5bdc
        checksum/config: 68a2e10b14482e5c16f5b615b53e496349433ea0236aeacc742afb4e2d21f6c4
    spec:
      securityContext:
        runAsUser: 1000
        runAsGroup: 1000
        fsGroup: 1000
        fsGroupChangePolicy: OnRootMismatch

      serviceAccountName: minio-sa
      containers:
        - name: minio
          image: quay.io/minio/minio:RELEASE.2022-09-17T00-09-45Z
          imagePullPolicy: IfNotPresent

          command: [ "/bin/sh",
            "-ce",
            "/usr/bin/docker-entrypoint.sh minio server  http://lgtm-minio-{0...0}.lgtm-minio-svc.lgtm-poc-8.svc.cluster.local/export-{0...1} -S /etc/minio/certs/ --address :9000 --console-address :9001" ]
          volumeMounts:
            - name: export-0
              mountPath: /export-0
            - name: export-1
              mountPath: /export-1            
          ports:
            - name: http
              containerPort: 9000
            - name: http-console
              containerPort: 9001
          env:
            - name: MINIO_ROOT_USER
              valueFrom:
                secretKeyRef:
                  name: lgtm-minio
                  key: rootUser
            - name: MINIO_ROOT_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: lgtm-minio
                  key: rootPassword
            - name: MINIO_PROMETHEUS_AUTH_TYPE
              value: "public"
          resources:
            requests:
              cpu: 50m
              memory: 128Mi      
      volumes:
        - name: minio-user
          secret:
            secretName: lgtm-minio        
  volumeClaimTemplates:
    - metadata:
        name: export-0
      spec:
        accessModes: [ "ReadWriteOnce" ]
        resources:
          requests:
            storage: 5Gi
    - metadata:
        name: export-1
      spec:
        accessModes: [ "ReadWriteOnce" ]
        resources:
          requests:
            storage: 5Gi
---
# Source: lgtm-distributed/charts/loki/templates/compactor/statefulset-compactor.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: lgtm-loki-compactor
  namespace: lgtm-poc-8
  labels:
    helm.sh/chart: loki-6.9.0
    app.kubernetes.io/name: loki
    app.kubernetes.io/instance: lgtm
    app.kubernetes.io/version: "3.1.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: compactor
    app.kubernetes.io/part-of: memberlist
spec:
  replicas: 1
  podManagementPolicy: Parallel
  updateStrategy:
    rollingUpdate:
      partition: 0
  serviceName: lgtm-loki-compactor-headless
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app.kubernetes.io/name: loki
      app.kubernetes.io/instance: lgtm
      app.kubernetes.io/component: compactor
  template:
    metadata:
      annotations:
        checksum/config: 2a79ed5c83e3dc3330ad5e8e316cc0319e73df426335c55911206c359308d362
      labels:
        app.kubernetes.io/name: loki
        app.kubernetes.io/instance: lgtm
        app.kubernetes.io/component: compactor
        app.kubernetes.io/part-of: memberlist
    spec:
      serviceAccountName: lgtm
      
      securityContext:
        fsGroup: 10001
        runAsGroup: 10001
        runAsNonRoot: true
        runAsUser: 10001
      terminationGracePeriodSeconds: 30
      containers:
        - name: compactor
          image: docker.io/grafana/loki:3.1.0
          imagePullPolicy: IfNotPresent
          args:
            - -config.file=/etc/loki/config/config.yaml
            - -target=compactor
          ports:
            - name: http-metrics
              containerPort: 3100
              protocol: TCP
            - name: grpc
              containerPort: 9095
              protocol: TCP
            - name: http-memberlist
              containerPort: 7946
              protocol: TCP
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          
          readinessProbe:
            httpGet:
              path: /ready
              port: http-metrics
            initialDelaySeconds: 30
            timeoutSeconds: 1
          
          volumeMounts:
            - name: temp
              mountPath: /tmp
            - name: config
              mountPath: /etc/loki/config
            - name: runtime-config
              mountPath: /etc/loki/runtime-config
            - name: data
              mountPath: /var/loki
          resources:
            limits:
              cpu: 200m
              memory: 128Mi
            requests:
              cpu: 50m
              memory: 128Mi
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                app.kubernetes.io/component: compactor
            topologyKey: kubernetes.io/hostname
      volumes:
        - name: temp
          emptyDir: {}
        - name: config
          configMap:
            name: loki
            items:
              - key: "config.yaml"
                path: "config.yaml"
        - name: runtime-config
          configMap:
            name: loki-runtime
        - name: data
          emptyDir: {}
---
# Source: lgtm-distributed/charts/loki/templates/index-gateway/statefulset-index-gateway.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: lgtm-loki-index-gateway
  labels:
    helm.sh/chart: loki-6.9.0
    app.kubernetes.io/name: loki
    app.kubernetes.io/instance: lgtm
    app.kubernetes.io/version: "3.1.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: index-gateway
spec:
  replicas: 1
  updateStrategy:
    rollingUpdate:
      partition: 0
  serviceName: lgtm-loki-index-gateway-headless
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app.kubernetes.io/name: loki
      app.kubernetes.io/instance: lgtm
      app.kubernetes.io/component: index-gateway
  template:
    metadata:
      annotations:
        checksum/config: 2a79ed5c83e3dc3330ad5e8e316cc0319e73df426335c55911206c359308d362
      labels:
        app.kubernetes.io/name: loki
        app.kubernetes.io/instance: lgtm
        app.kubernetes.io/component: index-gateway
        app.kubernetes.io/part-of: memberlist
    spec:
      serviceAccountName: lgtm
      
      securityContext:
        fsGroup: 10001
        runAsGroup: 10001
        runAsNonRoot: true
        runAsUser: 10001
      terminationGracePeriodSeconds: 300
      containers:
        - name: index-gateway
          image: docker.io/grafana/loki:3.1.0
          imagePullPolicy: IfNotPresent
          args:
            - -config.file=/etc/loki/config/config.yaml
            - -target=index-gateway
          ports:
            - name: http-metrics
              containerPort: 3100
              protocol: TCP
            - name: grpc
              containerPort: 9095
              protocol: TCP
            - name: http-memberlist
              containerPort: 7946
              protocol: TCP
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          readinessProbe:
            httpGet:
              path: /ready
              port: http-metrics
            initialDelaySeconds: 30
            timeoutSeconds: 1
          livenessProbe:
            null
          volumeMounts:
            - name: config
              mountPath: /etc/loki/config
            - name: runtime-config
              mountPath: /etc/loki/runtime-config
            - name: data
              mountPath: /var/loki
          resources:
            limits:
              cpu: 200m
              memory: 128Mi
            requests:
              cpu: 50m
              memory: 128Mi
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                app.kubernetes.io/component: index-gateway
            topologyKey: kubernetes.io/hostname
      volumes:
        - name: config
          configMap:
            name: loki
            items:
              - key: "config.yaml"
                path: "config.yaml"
        - name: runtime-config
          configMap:
            name: loki-runtime
        - name: data
          emptyDir: {}
---
# Source: lgtm-distributed/charts/loki/templates/ingester/statefulset-ingester.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: lgtm-loki-ingester
  namespace: lgtm-poc-8
  labels:
    helm.sh/chart: loki-6.9.0
    app.kubernetes.io/name: loki
    app.kubernetes.io/instance: lgtm
    app.kubernetes.io/version: "3.1.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: ingester
    app.kubernetes.io/part-of: memberlist
spec:
  replicas: 1
  podManagementPolicy: Parallel
  updateStrategy:
    rollingUpdate:
      partition: 0
  serviceName: lgtm-loki-ingester-headless
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app.kubernetes.io/name: loki
      app.kubernetes.io/instance: lgtm
      app.kubernetes.io/component: ingester
  template:
    metadata:
      annotations:
        checksum/config: 2a79ed5c83e3dc3330ad5e8e316cc0319e73df426335c55911206c359308d362
      labels:
        app.kubernetes.io/name: loki
        app.kubernetes.io/instance: lgtm
        app.kubernetes.io/component: ingester
        app.kubernetes.io/part-of: memberlist
    spec:
      topologySpreadConstraints:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/component: ingester
          maxSkew: 1
          topologyKey: kubernetes.io/hostname
          whenUnsatisfiable: ScheduleAnyway
      serviceAccountName: lgtm
      
      securityContext:
        fsGroup: 10001
        runAsGroup: 10001
        runAsNonRoot: true
        runAsUser: 10001
      terminationGracePeriodSeconds: 300
      containers:
        - name: ingester
          image: docker.io/grafana/loki:3.1.0
          imagePullPolicy: IfNotPresent
          args:
            - -config.file=/etc/loki/config/config.yaml
            - -ingester.availability-zone=zone-default
            - -target=ingester
          ports:
            - name: http-metrics
              containerPort: 3100
              protocol: TCP
            - name: grpc
              containerPort: 9095
              protocol: TCP
            - name: http-memberlist
              containerPort: 7946
              protocol: TCP
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          
          readinessProbe:
            httpGet:
              path: /ready
              port: http-metrics
            initialDelaySeconds: 30
            timeoutSeconds: 1
          
          volumeMounts:
            - name: config
              mountPath: /etc/loki/config
            - name: runtime-config
              mountPath: /etc/loki/runtime-config
            - name: data
              mountPath: /var/loki
          resources:
            limits:
              cpu: 200m
              memory: 128Mi
            requests:
              cpu: 50m
              memory: 128Mi
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                app.kubernetes.io/component: ingester
            topologyKey: kubernetes.io/hostname
      volumes:
        - name: config
          configMap:
            name: loki
            items:
              - key: "config.yaml"
                path: "config.yaml"
        - name: runtime-config
          configMap:
            name: loki-runtime
        - name: data
          emptyDir: { }
